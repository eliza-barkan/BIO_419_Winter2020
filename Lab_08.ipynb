{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Classification & PCA in Scikit-Learn\n",
    "**Data Science for Biologists** &#8226; University of Washington &#8226; BIOL 419/519 &#8226; Winter 2020\n",
    "\n",
    "Course design and lecture material by [Bingni Brunton](https://github.com/bwbrunton) and [Kameron Harris](https://github.com/kharris/). Lab design and materials by Eleanor Lutz and Eliza Barkan, with helpful comments and suggestions from Bing and Kam.\n",
    "\n",
    "### Table of Contents\n",
    "PART A: Practice Supervised Learning with Categorial Output\n",
    "1. Review of importing and inspecting data\n",
    "2. Split a dataset into a training and test set\n",
    "3. Train a machine learning classifier (linear discriminant analysis) using scikit-learn\n",
    "\n",
    "PART B: Practice Unsupervised Learning with Continuous Output \n",
    "4. Practice Principal Component Analysis (PCA)\n",
    "5. PCA by eigen decomposition\n",
    "6. PCA with Scikit-Learn\n",
    "\n",
    "PART C: Bonus Exercises: Practice Supervised Learning with Categorial Output\n",
    "7. Comparing different classifiers: LDA vs K-Nearest Neighbors\n",
    "\n",
    "### Helpful resources\n",
    "- [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas\n",
    "- [An introduction to machine learning with Scikit-Learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "- [Scikit-Learn user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Scikit-Learn Cheat Sheet](https://datacamp-community-prod.s3.amazonaws.com/5433fa18-9f43-44cc-b228-74672efcd116) by Python for Data Science\n",
    "- [Scikit-Learn PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) \n",
    "- [What is PCA?](https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial)\n",
    "- [PCA lecture](http://www.cse.psu.edu/~rtc12/CSE586Spring2010/lectures/pcaLectureShort_6pp.pdf) by Octavia Camps, PSU\n",
    "\n",
    "### Data\n",
    "- The data for part A & C is originally from [USA Forensic Science Service](https://archive.ics.uci.edu/ml/datasets/Glass+Identification) and was edited for teaching purposes. \n",
    "- The data for part B  was downloaded from [Kaggle](https://www.kaggle.com/uciml/iris) (originally from [Fisher 1936](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf)) and was edited for teaching purposes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's lab requires plotting several different classifications in different colors, so set the default matplotlib style to a colorblind-friendly setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: Practice Supervised Learning with Categorial Output\n",
    "## Section 1: Review of importing and inspecting data\n",
    "\n",
    "This week's data is from the USA Forensic Science Service, and contains information about 214 samples from seven different types of glass (vehicle windows, tableware, headlamps, etc). By analyzing various properties of the glass, such as the refractive index or the proportion of different chemical elements (Na, Mg, Al, etc.), this dataset can be used to predict the source of unknown glass found at crime scenes. \n",
    "\n",
    "**To import the data:** \n",
    "\n",
    "**Option 1** Download and install [Anaconda Navigator](https://www.anaconda.com/distribution/) to use an Anaconda Jupyter notebook. It works just like Google Colaboratory but you can much more easily import files directly from your local computer.\n",
    "\n",
    "**Option 2** Access files from Google Drive (using Google Colaboratory)\n",
    "1. Put files Iris.csv on your Google Drive\n",
    "2. Mount drive by selecting the file icon on the left and selecting \"Mount Drive\"\n",
    "3. Run the cell it creates and follow the links to get access to files on your Google Drive\n",
    "4. In a new cell, use \"cd\" (change directory) to go into a directory (aka a folder) or \"ls\" to see what is in the directory (aka a folder) that you are currently in. Continue using cd/ls to get to the folder on your Google Drive that contains these two csv files. If you get stuck, you can use \"cd ..\" to go backwards.\n",
    "5. You may then continue with the lab, starting with running pd.read_csv(filename)\n",
    "\n",
    "**Option 3** Open files from your local file system (using Google Colaboratory)\n",
    "1. Select <> icon above the file icon on the left side bar. \n",
    "2. Scroll through options until you get to \"Open files from your local file system,\" select this option and run the cell it creates. Find the files on your local computer\n",
    "3. You may then continue with the lab, starting with running pd.read_csv(filename)\n",
    "\n",
    "**Exercise 1:** Read in the dataset in the file `\"glass_properties_data.csv\"` as a `Pandas` dataframe called `glass_df`. `display()` the `head()` of the data and `describe()` the dataframe to make sure your `glass_df` variable was imported properly and has the expected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Split the dataset into a training set and test set\n",
    "\n",
    "**Using this dataset, we would like to create a machine learning classifier that can be used to categorize unknown glass samples.**\n",
    "\n",
    "A machine learning classifier is trained based on one particular set of data. A separate dataset is used to evaluate the accuracy of the classifier. For this lab we'll use 70% of the data for training and save the remaining 30% for testing the classifier. This means that we first need to separate a random 70% of the data into a `train_data` variable, and the other 30% into a different `test_data` variable. \n",
    "\n",
    "\n",
    "Below is a visualization showing how to:\n",
    "1. Divide training and test data  \n",
    "2. Train a classifier with the training data\n",
    "3. Assess the classifier with the testing data\n",
    "<img src=\"http://ataspinar.com/wp-content/uploads/2016/12/classification.png\" />\n",
    "\n",
    "*Credit:* A schematic overview of the classification process, by Ahmet Taspinar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data using Pandas\n",
    "\n",
    "`pandas` has a builtin function to sample a random subset of the dataset (as do other libraries including `scikit-learn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 70% of data from dataframe\n",
    "train_data = glass_df.sample(frac=0.7)\n",
    "\n",
    "# drop all rows in train_data and assign the rest to test_data\n",
    "test_data = glass_df.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Supervised Learning: train a machine learning classifier using scikit-learn\n",
    "\n",
    "Like `numpy` or `pandas`, [scikit-Learn](https://scikit-learn.org/stable/) is a python library for machine learning. In today's lab we will make a Linear Discriminant Analysis classifier by importing this function from the `sklearn.discriminant_analysis` module. \n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "In class we discussed the Linear Discriminant Analysis method for two variables. The scikit-learn library can also extrapolate this principle to run a LDA on many variables. To run a LDA on our dataset we will import the `LDA` function from the Scikit learn (`sklearn`) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `LinearDiscriminantAnalysis` function we need to first separate the data into the data values and the classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = train_data[\"Glass_Type\"] # known classification answers\n",
    "train_vals = train_data.drop([\"Glass_Type\", \"ID\"], axis=1) # corresponding values \n",
    "# make sure to drop the ID since it is not a characteristic of the glass\n",
    "\n",
    "display(train_class.head())\n",
    "display(train_vals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Similarly, split the `test_data` test dataset into a variable called `test_class` that holds all of the `Glass_Type` classifications, and another variable called `test_vals` that contains the corresponding measurement values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training data is separated into the values and the known classifier values, these two dataframes can be used to train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the classifier in notebook\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "# train the classifier with our training data\n",
    "classifier.fit(train_vals, train_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the classifier, we can use it to predict the types of classes for the training data, and see what proportion of the predictions matched the correct answers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = classifier.score(train_vals, train_class)\n",
    "print(\"Training score is:\", train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier can choose between 7 different glass types, so anything above 1/7 (or 14.29%) is better than chance. The classifier should be performing much better than 14% on the training dataset we scored above. \n",
    "\n",
    "But the real evaluation of the classifier is to test its accuracy in predicting the glass classes for data it has never seen before:\n",
    "\n",
    "**Exercise 3**: Assess the accuracy of your model on the test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Compare your test score with two other people in the class. \n",
    "- Did you get the same accuracy values? If not, why do you think this might be the case? Explain briefly as a comment below.\n",
    "- Is the accuracy of your classifier better or worse for the test data or the training data? Why do you think this is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B: Practice Unsupervised Learning with Continuous Output\n",
    "## Section 1: Unsupervised Learning: Practice Principal Component Analysis\n",
    "\n",
    "So far we have practiced everything below in bold, however we have not yet practiced what to do in the scenario where we do not have data labels and we want a continuous output. In this situation there are different ways to find patterns in the data, one of these is the dimensional reduction method principal component analysis or 'PCA'. \n",
    "\n",
    "Supervised (using known labels to predict labels of unknown data): \n",
    "1. **Continuous output**: Regression\n",
    "2. **Categorical Output**: k-nearest neighbor classifier & linear discriminant analysis classifier\n",
    "\n",
    "Unsupervised (looking for patterns in data): \n",
    "1. Continuous output: Various (e.g. principal component analysis)  <---\n",
    "2. **Categorical Output**: k-means clustering\n",
    "\n",
    "Today, we want to see what patterns emerge from our iris data when we apply unsupervised learning techniques of PCA.\n",
    "\n",
    "**Exercise 5:** Read in the dataset in the file `\"iris_data.csv\"` as a `Pandas` dataframe called `iris_df`. `display()` the `head()` of the data and `describe()` the dataframe to make sure your `iris_df` variable was imported properly and has the expected 5 columns and 150 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: PCA by eigen decomposition\n",
    "\n",
    "**Exercise 6:** Create an array, `iris_values_only`, containing only the numerical columns from the iris data frame. Convert all values in the matrix to the Python `float` type. Then `display()` `iris_values_only`.\n",
    "\n",
    "Hint 1: `dataframe.values[row index, col index]` can be used to select only the values in a pandas dataframe.\n",
    "\n",
    "Hint 2: `.astype(type)` can be used to convert an object to a particular data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this matrix should have 150 rows and 4 columns. Check if your matrix matches these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(iris_values_only.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can calculate the covariance matrix for the iris data. This symmetrical matrix will tell us how similar the features are to each other. Higher values indicates a larger positive relationship between the features. More negative values indicate a larger negative relationship between the features. Diagonal features will tell you the variance of a particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_iris = iris_values_only.T.dot(iris_values_only) / iris_values_only.shape[0]\n",
    "display(cov_iris)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can perform eigen decomposition on the covariance matrix to identify the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eig function returns an array of eigenvalues and eigenvectors.\n",
    "eigenvectors = np.linalg.eig(cov_iris)[1]\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can multiply the iris data values by the eigenvectors to calculate the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = iris_values_only.dot(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PC1 by PC2\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.scatter(pc[:, 0], pc[:, 1])\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: PCA with Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Use scikit-learn's `PCA` implementation to fit a model to the iris data matrix, `iris_values_only`. See the [PCA documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) examples to get started. You do not need to specify the components because the default behavior is to fit using all components, which is what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PCA in notebook\n",
    "pca = PCA()\n",
    "\n",
    "# fit PCA with our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Plot the proportion of variance explained by each component.\n",
    "\n",
    "Hint: See the [PCA documentation](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) examples for variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can calculate the principal components from the PCA model using the `transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_transformed = pca.transform(iris_values_only)\n",
    "iris_transformed_df = pd.DataFrame(iris_transformed, columns=[\"pc1\", \"pc2\", \"pc3\", \"pc4\"])\n",
    "iris_transformed_df[\"species\"] = iris_df[\"variety\"]\n",
    "iris_transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Plot the PC transformed iris data for the first two principal components with PC1 on the x axis and PC2 on the y axis. Color points by species. Label axes and include percentage of variance explained for each component in the axis label.\n",
    "\n",
    "How well did PCA identify separate clusters per species? Explain briefly as a comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Run the code in the next cell and then discuss with your partner how well PC1 and PC2 separated by species above compared to any of the features plotted below? Explain briefly as a comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "df_subset = iris_df.loc[:,:\"variety\"]\n",
    "sns.pairplot(df_subset, hue=\"variety\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11:** You may notice that PC1 explains a lot of the variance. Let's see how much of each pc is explained by each of the original features by running the code below. Discuss with your partner what these results mean. Explain briefly as a comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify\n",
    "pc_explained = pd.DataFrame(pca.components_, columns=iris_df.columns[:4])\n",
    "display(pc_explained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART C: Bonus Exercises: Practice Supervised Learning with Categorial Output\n",
    "\n",
    "### Comparing different classifiers: LDA vs K-Nearest Neighbors\n",
    "In class we discussed Nearest Neighbor classification, which classifies an unknown data point as belonging to the same class as its nearest neighboring point. K-Nearest Neighbors extrapolates this approach to more than one neighbor. For example, a 3-nearest neighbor classification looks at the three nearest data points, and picks the most common class from among those three neighbors. \n",
    "\n",
    "In `sklearn` a KNN classifier can be imported from the `sklearn.neighbors` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Exercise 1:** Evaluate the accuracy of `KNeighborsClassifier()` using the same training and test dataset as used previously in part A of this lab. Is this classification method better than LDA for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Exercise 2:** Look up the documentation for `sklearn KNeighborsClassifier()` to find out how to specify the number of neighbors to use. Make a scatterplot showing the accuracy of the classifier on the test dataset for a range of n-nearest neighbors (1 to 50). How does the number of nearest neighbors affect classifier accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
